{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFe+HvOpVV+EEvMbJgWbvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramadeviballa1-arch/Health_ai_app/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrlY_CjFW_Eu"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok transformers accelerate PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Set your Hugging Face API key here\n",
        "os.environ[\"HF_TOKEN\"] =\" your_token \""
      ],
      "metadata": {
        "id": "3zURny0fXIN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from PyPDF2 import PdfReader\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"ü©∫ Health AI\", layout=\"centered\")\n",
        "\n",
        "# Load model\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model_name = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        use_auth_token=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = load_model()\n",
        "\n",
        "# Extract text from PDF\n",
        "def extract_text_from_pdf(uploaded_file):\n",
        "    reader = PdfReader(uploaded_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        if page.extract_text():\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"üè• Health AI\")\n",
        "page = st.sidebar.radio(\"Choose an Option\", [\"Medical Report\", \"Home Remedies\", \"Medical Chatbot\"])\n",
        "\n",
        "# ---------- Medical Report Analysis ----------\n",
        "if page == \"Medical Report\":\n",
        "    st.title(\" Medical Report Analyzer\")\n",
        "    uploaded_file = st.file_uploader(\"Upload a medical report (PDF)\", type=\"pdf\")\n",
        "\n",
        "    if uploaded_file:\n",
        "        report_text = extract_text_from_pdf(uploaded_file)\n",
        "        st.success(\" Report uploaded.\")\n",
        "\n",
        "        if st.button(\" Analyze Report\"):\n",
        "            with st.spinner(\"Analyzing the medical report...\"):\n",
        "                prompt = f\"\"\"\n",
        "Analyze this medical report and give a detailed summary in a simple way.\n",
        "Also based on the report, generate the health condition.\n",
        "Here is the report:\n",
        "{report_text[:3500]}\n",
        "\"\"\"\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=768, do_sample=True, temperature=0.7)\n",
        "                result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                st.subheader(\"ü©∫  Patient Report Summary\")\n",
        "                st.write(result)\n",
        "\n",
        "        st.subheader(\"üí¨ Ask a Question about the Report\")\n",
        "        user_query = st.text_input(\"Ask your question...\")\n",
        "        if user_query:\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                qa_prompt = f\"Based on this report:\\n{report_text[:3000]}\\n\\nAnswer this question in a simple way: {user_query}\"\n",
        "                inputs = tokenizer(qa_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.7)\n",
        "                answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                st.markdown(\"ü§ñ Answer:\")\n",
        "                st.write(answer)\n",
        "\n",
        "# ---------- Home Remedies ----------\n",
        "elif page == \"Home Remedies\":\n",
        "    st.title(\" Home Remedies & Self-Care Assistant\")\n",
        "    symptoms = st.text_area(\"Enter your symptoms (e.g., fever, cough, sore throat)\", height=100)\n",
        "\n",
        "    if st.button(\" Get Remedies & Self-Care Tips\"):\n",
        "        with st.spinner(\"Fetching personalized suggestions...\"):\n",
        "            prompt = f\"\"\"\n",
        "I have the following symptoms: {symptoms}\n",
        "\n",
        "1. Suggest at least 10 simple and natural home remedies for the symptoms.\n",
        "2. Also provide at least 10 self-care and lifestyle tips to manage or relieve the symptoms.\n",
        "\"\"\"\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=0.8)\n",
        "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            if \"üßò\" in result:\n",
        "                remedies_text, selfcare_text = result.split(\"üßò\", 1)\n",
        "\n",
        "                st.markdown(\"<h3><b>1.  Home Remedies </b></h3>\", unsafe_allow_html=True)\n",
        "                st.write(remedies_text.strip())\n",
        "\n",
        "                st.markdown(\"<h3><b>2.  Self-Care and Lifestyle Tips</b></h3>\", unsafe_allow_html=True)\n",
        "                st.write(selfcare_text.strip())\n",
        "            else:\n",
        "                st.write(result)\n",
        "\n",
        "# ---------- Medical Chatbot ----------\n",
        "elif page == \"Medical Chatbot\":\n",
        "    st.title(\" AI Medical Chatbot\")\n",
        "    st.markdown(\"Ask any general medical question below and get an AI-powered response.\")\n",
        "\n",
        "    # Initialize chat history\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    # Chat input\n",
        "    user_query = st.chat_input(\"Ask a medical question...\")\n",
        "\n",
        "    if user_query:\n",
        "        with st.spinner(\"Generating response...\"):\n",
        "            prompt = f\"Answer the following medical question.:\\n\\n{user_query}\"\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
        "            answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Save interaction\n",
        "            st.session_state.chat_history.append((user_query, answer))\n",
        "\n",
        "    # Display chat history\n",
        "    for user_msg, bot_msg in st.session_state.chat_history:\n",
        "        st.chat_message(\"user\").write(user_msg)\n",
        "        st.chat_message(\"assistant\").write(bot_msg)"
      ],
      "metadata": {
        "id": "04ZnIS4nXLre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken   \"your_token\""
      ],
      "metadata": {
        "id": "nMGId-WEXSZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null &"
      ],
      "metadata": {
        "id": "rp1oi1e3XTuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\" Streamlit App is Live at: {public_url}\")"
      ],
      "metadata": {
        "id": "2MYA2WrzXtGw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}